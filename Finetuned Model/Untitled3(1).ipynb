{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_28o2dLXnmAH",
        "outputId": "fb2508d5-ec1c-4870-9655-c471f27796a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.33.2\n"
          ]
        }
      ],
      "source": [
        "import google.protobuf\n",
        "print(google.protobuf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n1uQa5aVhsc7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import json\n",
        "import yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y7Qp3VvLjDeS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 800 rows from ./ci_cd_logs.csv\n",
            "\n",
            "Columns: ['timestamp', 'pipeline_id', 'stage_name', 'job_name', 'task_name', 'status', 'message', 'commit_id', 'branch', 'user', 'environment']\n",
            "\n",
            "Data types:\n",
            "timestamp      object\n",
            "pipeline_id    object\n",
            "stage_name     object\n",
            "job_name       object\n",
            "task_name      object\n",
            "status         object\n",
            "message        object\n",
            "commit_id      object\n",
            "branch         object\n",
            "user           object\n",
            "environment    object\n",
            "dtype: object\n",
            "\n",
            "Status distribution:\n",
            "status\n",
            "skipped    219\n",
            "running    206\n",
            "failed     194\n",
            "success    181\n",
            "Name: count, dtype: int64\n",
            "\n",
            "First few rows:\n",
            "                  timestamp pipeline_id stage_name           job_name  \\\n",
            "0  2024-03-02 01:05:07+0000  pipe-txnem      Build  deploy_to_staging   \n",
            "1  2024-07-22 19:55:41+0000  pipe-hjahz      Build     run_unit_tests   \n",
            "2  2024-03-01 23:03:43+0000  pipe-vcsbx   Analysis      deploy_to_dev   \n",
            "3  2024-06-02 12:21:00+0000  pipe-pnvzk       Test      deploy_to_dev   \n",
            "4  2024-04-17 07:59:29+0000  pipe-mwkkl       Test     build_and_test   \n",
            "\n",
            "  task_name   status                                       message  \\\n",
            "0   analyze  success                  Task completed successfully.   \n",
            "1    deploy  skipped  Task was skipped due to pipeline conditions.   \n",
            "2    deploy  success                  Task completed successfully.   \n",
            "3      test  skipped  Task was skipped due to pipeline conditions.   \n",
            "4      test   failed                        Task execution failed.   \n",
            "\n",
            "                                  commit_id      branch      user environment  \n",
            "0  f831dbe56dcbceadccc1447923b1d9659becadbe  branch_ajn       NaN         NaN  \n",
            "1  dcbac61f342d8b5ed473959be9edbeb1dfaca028  branch_tgn  user_psc         NaN  \n",
            "2  fded3f1e6bcedb9fc338c14dfad3aa360bb6b50c  branch_lti  user_usq         NaN  \n",
            "3  2c9d4fbd41bb1c5c7feb06035d3b4feaf1ac45d0  branch_ezx       NaN         NaN  \n",
            "4  a4e872fbdf81f1d1742bf5ae0c76ee9d10eea02a  branch_xqp  user_umu         NaN  \n"
          ]
        }
      ],
      "source": [
        "csv_file = \"./ci_cd_logs.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "print(f\"Loaded {len(df)} rows from {csv_file}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nData types:\\n{df.dtypes}\")\n",
        "print(f\"\\nStatus distribution:\\n{df['status'].value_counts()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Found 194 failed pipeline logs\n",
            "\n",
            "‚úÖ Created ./train.jsonl with 194 training examples\n",
            "\n",
            "üìã Sample training example:\n",
            "{\n",
            "  \"messages\": [\n",
            "    {\n",
            "      \"role\": \"user\",\n",
            "      \"content\": \"Analyze this CI/CD pipeline failure and provide a solution:\\n\\nPipeline ID: pipe-mwkkl\\nStage: Test\\nJob: build_and_test\\nTask: test\\nBranch: branch_xqp\\nStatus: failed\\nError: Task execution failed.\\n\\nWhat went wrong and how should we fix it?\"\n",
            "    },\n",
            "    {\n",
            "      \"role\": \"assistant\",\n",
            "      \"content\": \"Tests failed in the Test stage.\\n\\nRecommended fixes:\\n1. Review the test failure logs for specific assertions\\n2. Check if recent code changes in commit a4e872fb broke functionality\\n3. Verify test data and mocks are properly configured\\n4. Run tests locally to reproduce and debug\\n5. Check for environment-specific issues in nan\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "jsonl_file = \"./train.jsonl\"\n",
        "\n",
        "# Filter for failures (you can adjust this filter)\n",
        "failed_logs = df[df['status'].str.lower().isin(['failed', 'error', 'failure'])].copy()\n",
        "print(f\"\\nFound {len(failed_logs)} failed pipeline logs\")\n",
        "\n",
        "if len(failed_logs) == 0:\n",
        "    print(\"‚ö†Ô∏è  No failed logs found. Using all logs for training.\")\n",
        "    failed_logs = df.copy()\n",
        "\n",
        "# Create training examples\n",
        "training_examples = []\n",
        "\n",
        "for idx, row in failed_logs.iterrows():\n",
        "    # Build a context-rich instruction\n",
        "    context_parts = []\n",
        "    \n",
        "    # Add each field if it's not null\n",
        "    if pd.notna(row['pipeline_id']):\n",
        "        context_parts.append(f\"Pipeline ID: {row['pipeline_id']}\")\n",
        "    if pd.notna(row['stage_name']):\n",
        "        context_parts.append(f\"Stage: {row['stage_name']}\")\n",
        "    if pd.notna(row['job_name']):\n",
        "        context_parts.append(f\"Job: {row['job_name']}\")\n",
        "    if pd.notna(row['task_name']):\n",
        "        context_parts.append(f\"Task: {row['task_name']}\")\n",
        "    if pd.notna(row['branch']):\n",
        "        context_parts.append(f\"Branch: {row['branch']}\")\n",
        "    if pd.notna(row['environment']):\n",
        "        context_parts.append(f\"Environment: {row['environment']}\")\n",
        "    if pd.notna(row['status']):\n",
        "        context_parts.append(f\"Status: {row['status']}\")\n",
        "    if pd.notna(row['message']):\n",
        "        context_parts.append(f\"Error: {row['message']}\")\n",
        "    \n",
        "    context = \"\\n\".join(context_parts)\n",
        "    \n",
        "    # Create instruction\n",
        "    instruction = f\"\"\"Analyze this CI/CD pipeline failure and provide a solution:\n",
        "\n",
        "{context}\n",
        "\n",
        "What went wrong and how should we fix it?\"\"\"\n",
        "    \n",
        "    # Create a realistic response (you'll want to enhance this with actual solutions)\n",
        "    # For now, creating template responses based on common CI/CD issues\n",
        "    message_lower = str(row['message']).lower()\n",
        "    \n",
        "    # Pattern matching for common CI/CD errors\n",
        "    if any(word in message_lower for word in ['timeout', 'timed out']):\n",
        "        solution = f\"\"\"The pipeline timed out in the {row['stage_name']} stage. \n",
        "\n",
        "Recommended fixes:\n",
        "1. Increase timeout values in your CI/CD configuration\n",
        "2. Optimize the {row['task_name']} task to run faster\n",
        "3. Check for network issues or slow dependencies\n",
        "4. Consider parallelizing tasks if possible\"\"\"\n",
        "    \n",
        "    elif any(word in message_lower for word in ['test', 'failed', 'assertion']):\n",
        "        solution = f\"\"\"Tests failed in the {row['stage_name']} stage.\n",
        "\n",
        "Recommended fixes:\n",
        "1. Review the test failure logs for specific assertions\n",
        "2. Check if recent code changes in commit {row['commit_id'][:8]} broke functionality\n",
        "3. Verify test data and mocks are properly configured\n",
        "4. Run tests locally to reproduce and debug\n",
        "5. Check for environment-specific issues in {row['environment']}\"\"\"\n",
        "    \n",
        "    elif any(word in message_lower for word in ['dependency', 'package', 'module', 'import']):\n",
        "        solution = f\"\"\"Dependency or import error in {row['stage_name']}.\n",
        "\n",
        "Recommended fixes:\n",
        "1. Update your requirements.txt or package.json with correct versions\n",
        "2. Clear dependency cache and reinstall\n",
        "3. Check for version conflicts between packages\n",
        "4. Verify all required dependencies are listed in your config files\n",
        "5. Consider using a dependency lock file\"\"\"\n",
        "    \n",
        "    elif any(word in message_lower for word in ['permission', 'denied', 'unauthorized']):\n",
        "        solution = f\"\"\"Permission or access error in {row['stage_name']}.\n",
        "\n",
        "Recommended fixes:\n",
        "1. Check CI/CD service account permissions\n",
        "2. Verify secrets and credentials are properly configured\n",
        "3. Review branch protection rules for {row['branch']}\n",
        "4. Ensure user {row['user']} has necessary access rights\n",
        "5. Check file/directory permissions in the {row['environment']} environment\"\"\"\n",
        "    \n",
        "    elif any(word in message_lower for word in ['build', 'compile', 'syntax']):\n",
        "        solution = f\"\"\"Build or compilation error in {row['stage_name']}.\n",
        "\n",
        "Recommended fixes:\n",
        "1. Check syntax errors in recent commit {row['commit_id'][:8]}\n",
        "2. Verify build configuration files are correct\n",
        "3. Ensure all build dependencies are available\n",
        "4. Check for environment-specific compilation issues\n",
        "5. Review build logs for specific error details\"\"\"\n",
        "    \n",
        "    elif any(word in message_lower for word in ['deploy', 'deployment']):\n",
        "        solution = f\"\"\"Deployment failed in {row['environment']} environment.\n",
        "\n",
        "Recommended fixes:\n",
        "1. Verify deployment credentials and configurations\n",
        "2. Check if the {row['environment']} environment is accessible\n",
        "3. Review resource availability (disk space, memory)\n",
        "4. Validate deployment manifests and configurations\n",
        "5. Check for conflicts with existing deployments\"\"\"\n",
        "    \n",
        "    else:\n",
        "        solution = f\"\"\"The pipeline failed in the {row['stage_name']} stage with status: {row['status']}.\n",
        "\n",
        "Recommended fixes:\n",
        "1. Review the full error message: \"{row['message']}\"\n",
        "2. Check logs for commit {row['commit_id'][:8]} on branch {row['branch']}\n",
        "3. Verify the {row['task_name']} task configuration\n",
        "4. Test the build locally in a similar environment\n",
        "5. Check recent changes by user {row['user']}\n",
        "6. Review {row['environment']} environment settings\"\"\"\n",
        "    \n",
        "    # Create conversation format\n",
        "    example = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": instruction},\n",
        "            {\"role\": \"assistant\", \"content\": solution}\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    training_examples.append(example)\n",
        "\n",
        "# Write to JSONL\n",
        "with open(jsonl_file, 'w') as f:\n",
        "    for example in training_examples:\n",
        "        f.write(json.dumps(example) + '\\n')\n",
        "\n",
        "print(f\"\\n‚úÖ Created {jsonl_file} with {len(training_examples)} training examples\")\n",
        "\n",
        "# Show a sample\n",
        "print(\"\\nüìã Sample training example:\")\n",
        "print(json.dumps(training_examples[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tqVTBdVm6fv",
        "outputId": "844e1ee7-556b-4b41-ca50-f6a4d1937c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.33.2\n"
          ]
        }
      ],
      "source": [
        "import google.protobuf\n",
        "print(google.protobuf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbHHp2nonB6v"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# pip uninstall protobuf -y\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjgRO4tpjfHS",
        "outputId": "7ecf91e5-e3cb-4604-aec6-87cd6fd7d094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ YAML config saved to ./train.yaml\n",
            "\n",
            "üìù Key change: dataset_name is now 'text_sft' (not 'oumi_sft')\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to create the corrected YAML config\n",
        "yaml_content = f\"\"\"\n",
        "model:\n",
        "  model_name: \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "  model_max_length: 1024\n",
        "  torch_dtype_str: \"bfloat16\"\n",
        "  trust_remote_code: true\n",
        "  attn_implementation: \"sdpa\"\n",
        "  load_pretrained_weights: true\n",
        "\n",
        "data:\n",
        "  train:\n",
        "    datasets:\n",
        "      - dataset_name: \"text_sft\"\n",
        "        dataset_path: \"{jsonl_file}\"\n",
        "        split: \"train\"\n",
        "\n",
        "peft:\n",
        "  lora_r: 16\n",
        "  lora_alpha: 32\n",
        "  lora_dropout: 0.05\n",
        "  lora_target_modules:\n",
        "    - q_proj\n",
        "    - v_proj\n",
        "    - k_proj\n",
        "    - o_proj\n",
        "\n",
        "training:\n",
        "  trainer_type: \"TRL_SFT\"\n",
        "  use_peft: true\n",
        "  per_device_train_batch_size: 4\n",
        "  gradient_accumulation_steps: 2\n",
        "  learning_rate: 3e-4\n",
        "  max_steps: {min(1000, len(training_examples) * 3)}\n",
        "  logging_steps: 10\n",
        "  save_steps: 200\n",
        "  run_name: \"cicd_auto_healer\"\n",
        "  output_dir: \"./output\"\n",
        "  save_final_model: true\n",
        "  warmup_steps: 100\n",
        "  lr_scheduler_type: \"cosine\"\n",
        "  weight_decay: 0.01\n",
        "\"\"\"\n",
        "\n",
        "yaml_path = \"./train.yaml\"\n",
        "with open(yaml_path, \"w\") as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(f\"‚úÖ YAML config saved to {yaml_path}\")\n",
        "print(\"\\nüìù Key change: dataset_name is now 'text_sft' (not 'oumi_sft')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9-OkJTHjy1l"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade protobuf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4Pzk1Kgj4jR",
        "outputId": "338e6b11-32ef-4554-af56-8d11bbfd6c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.33.2\n"
          ]
        }
      ],
      "source": [
        "import google.protobuf\n",
        "print(google.protobuf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "KfoPIeYqsnlW",
        "outputId": "1aa6be8c-ec55-4e89-8032-ad553b2184b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mnotebook controller is DISPOSED. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "pip install -U protobuf>=6.32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "_slBjnDbjlYx",
        "outputId": "75fb12b1-a800-46dc-ce2b-e233887965cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Training Configuration:\n",
            "  Model: HuggingFaceTB/SmolLM2-135M-Instruct\n",
            "  Max length: 1024\n",
            "  Dataset: 194 examples\n",
            "  Using PEFT: True\n",
            "  LoRA rank: 16\n",
            "  Batch size: 4\n",
            "  Max steps: 582\n",
            "  Output: ./output\n",
            "\n",
            "üöÄ Starting training...\n",
            "[2025-12-14 13:17:29,047][oumi][rank0][pid:33000][MainThread][INFO]][torch_utils.py:80] Torch version: 2.8.0+cu128. NumPy version: 2.2.6\n",
            "[2025-12-14 13:17:29,048][oumi][rank0][pid:33000][MainThread][INFO]][torch_utils.py:82] CUDA is not available!\n",
            "[2025-12-14 13:17:29,050][oumi][rank0][pid:33000][MainThread][INFO]][train.py:154] Oumi version: 0.5.0\n",
            "[2025-12-14 13:17:29,068][oumi][rank0][pid:33000][MainThread][INFO]][train.py:318] Training config saved to output/telemetry/training_config.yaml\n",
            "[2025-12-14 13:17:29,610][oumi][rank0][pid:33000][MainThread][INFO]][models.py:544] Using the model's built-in chat template for model 'HuggingFaceTB/SmolLM2-135M-Instruct'.\n",
            "[2025-12-14 13:17:29,612][oumi][rank0][pid:33000][MainThread][INFO]][base_map_dataset.py:91] Creating map dataset (type: TextSftJsonLinesDataset)... dataset_name: 'text_sft'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 9 examples [00:00, 832.26 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-14 13:17:29,647][oumi][rank0][pid:33000][MainThread][INFO]][base_map_dataset.py:312] TextSftJsonLinesDataset: features=dict_keys(['input_ids', 'attention_mask'])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating train split: 194 examples [00:00, 1945.14 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-14 13:17:29,782][oumi][rank0][pid:33000][MainThread][INFO]][base_map_dataset.py:376] Finished transforming dataset (TextSftJsonLinesDataset)! Speed: 1444.58 examples/sec. Examples: 194. Duration: 0.1 sec. Transform workers: 1.\n",
            "[2025-12-14 13:17:29,785][oumi][rank0][pid:33000][MainThread][INFO]][models.py:260] Building model using device_map: auto (DeviceRankInfo(world_size=1, rank=0, local_world_size=1, local_rank=0))...\n",
            "[2025-12-14 13:17:29,786][oumi][rank0][pid:33000][MainThread][INFO]][models.py:336] Using model class: <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> to instantiate model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-14 13:17:30,619][oumi][rank0][pid:33000][MainThread][INFO]][train.py:463] Building PEFT model...\n",
            "[2025-12-14 13:17:30,730][oumi][rank0][pid:33000][MainThread][INFO]][torch_utils.py:288] \n",
            "Model Parameters Summary:\n",
            "üî¢ Total     parameters: 136,358,208\n",
            "üîó Embedding parameters: 28,311,552\n",
            "üéØ Trainable parameters: 1,843,200\n",
            "üîí Frozen    parameters: 134,515,008 (98.65%)\n",
            "\n",
            "[2025-12-14 13:17:30,731][oumi][rank0][pid:33000][MainThread][INFO]][train.py:486] Skipping dataset preparation for TRL_SFT trainer since the dataset is already processed.\n",
            "[2025-12-14 13:17:31,000][oumi][rank0][pid:33000][MainThread][INFO]][torch_profiler_utils.py:164] PROF: Torch Profiler disabled!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-14 13:17:31,100][oumi][rank0][pid:33000][MainThread][INFO]][device_utils.py:343] GPU Metrics Before Training: GPU runtime info: None.\n",
            "[2025-12-14 13:17:31,101][oumi][rank0][pid:33000][MainThread][INFO]][train.py:558] Training init time: 2.055s\n",
            "[2025-12-14 13:17:31,102][oumi][rank0][pid:33000][MainThread][INFO]][train.py:559] Starting training... (TrainerType.TRL_SFT, transformers: 4.57.1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='582' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  5/582 12:05 < 38:45:03, 0.00 it/s, Epoch 0.16/24]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müöÄ Starting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Fine-tuning complete! Model saved in ./output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/oumi/train.py:564\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config, additional_model_kwargs, additional_trainer_kwargs, additional_tuning_kwargs, verbose)\u001b[0m\n\u001b[1;32m    558\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining init time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m_START_TIME\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    559\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    560\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training... \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mtrainer_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         )\n\u001b[0;32m--> 564\u001b[0m         \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining is Complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    568\u001b[0m log_nvidia_gpu_runtime_info(log_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Metrics After Training:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/oumi/core/trainers/hf_trainer.py:41\u001b[0m, in \u001b[0;36mHuggingFaceTrainer.train\u001b[0;34m(self, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume_from_checkpoint: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Trains a model.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:1190\u001b[0m, in \u001b[0;36mSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m-> 1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/transformers/trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 4071\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/accelerate/accelerator.py:2852\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2852\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venvs/ai310/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from oumi.core.configs import TrainingConfig\n",
        "from oumi.train import train\n",
        "\n",
        "# Fix HF warnings\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "try:\n",
        "    # Load config\n",
        "    config = TrainingConfig.from_yaml(yaml_path)\n",
        "    \n",
        "    print(\"\\nüìã Training Configuration:\")\n",
        "    print(f\"  Model: {config.model.model_name}\")\n",
        "    print(f\"  Max length: {config.model.model_max_length}\")\n",
        "    print(f\"  Dataset: {len(training_examples)} examples\")\n",
        "    print(f\"  Using PEFT: {config.training.use_peft}\")\n",
        "    print(f\"  LoRA rank: {config.peft.lora_r}\")\n",
        "    print(f\"  Batch size: {config.training.per_device_train_batch_size}\")\n",
        "    print(f\"  Max steps: {config.training.max_steps}\")\n",
        "    print(f\"  Output: {config.training.output_dir}\")\n",
        "    \n",
        "    # Start training\n",
        "    print(\"\\nüöÄ Starting training...\")\n",
        "    train(config)\n",
        "    \n",
        "    print(\"\\n‚úÖ Fine-tuning complete! Model saved in ./output\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error during training: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kcodess/venvs/ai310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading your fine-tuned CI/CD Auto-Healer model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/home/kcodess/venvs/ai310/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model loaded successfully!\n",
            "üìç Model location: ./output\n",
            "üß† Model size: 136.4M parameters\n",
            "üíæ Device: cpu\n"
          ]
        }
      ],
      "source": [
        "print(\"üîÑ Loading your fine-tuned CI/CD Auto-Healer model...\")\n",
        "\n",
        "model_path = \"./output\"  # Your trained model location\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"üìç Model location: {model_path}\")\n",
        "print(f\"üß† Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
        "print(f\"üíæ Device: {model.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading your fine-tuned CI/CD Auto-Healer model...\n",
            "‚úÖ Model loaded successfully!\n",
            "\n",
            "üß™ Testing with real pipeline failures from your dataset:\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üìã TEST 1: Test Stage Failure\n",
            "--------------------------------------------------------------------------------\n",
            "Response length: 334 chars\n",
            "\n",
            "assistant\n",
            "Tests failed in the Test stage.\n",
            "\n",
            "Recommended fixes:\n",
            "1. Review the test failure logs for specific assertions\n",
            "2. Check if recent code changes in commit 8f7a2c9e broke functionality\n",
            "3. Verify test data and mocks are properly configured\n",
            "4. Run tests locally to reproduce and debug\n",
            "5. Check for environment-specific issues in nan\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üìã TEST 2: Build Stage Failure\n",
            "--------------------------------------------------------------------------------\n",
            "Response length: 335 chars\n",
            "\n",
            "assistant\n",
            "Tests failed in the Build stage.\n",
            "\n",
            "Recommended fixes:\n",
            "1. Review the test failure logs for specific assertions\n",
            "2. Check if recent code changes in commit 97c1c47a broke functionality\n",
            "3. Verify test data and mocks are properly configured\n",
            "4. Run tests locally to reproduce and debug\n",
            "5. Check for environment-specific issues in nan\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üìã TEST 3: Deployment Failure\n",
            "--------------------------------------------------------------------------------\n",
            "Response length: 336 chars\n",
            "\n",
            "assistant\n",
            "Tests failed in the Deploy stage.\n",
            "\n",
            "Recommended fixes:\n",
            "1. Review the test failure logs for specific assertions\n",
            "2. Check if recent code changes in commit 7f15b6e6 broke functionality\n",
            "3. Verify test data and mocks are properly configured\n",
            "4. Run tests locally to reproduce and debug\n",
            "5. Check for environment-specific issues in nan\n",
            "\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Testing complete!\n",
            "\n",
            "üí° To test with your own failures, use:\n",
            "   diagnosis = diagnose_pipeline_failure(pipeline_id, stage, job, task, branch, status, message)\n"
          ]
        }
      ],
      "source": [
        "# Add this to a new cell in your notebook\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 1. Load your fine-tuned model\n",
        "print(\"üîÑ Loading your fine-tuned CI/CD Auto-Healer model...\")\n",
        "\n",
        "model_path = \"./output\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Set padding token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully!\\n\")\n",
        "\n",
        "# 2. Create a test function with better response extraction\n",
        "def diagnose_pipeline_failure(\n",
        "    pipeline_id,\n",
        "    stage_name,\n",
        "    job_name,\n",
        "    task_name,\n",
        "    branch,\n",
        "    status,\n",
        "    message,\n",
        "    commit_id=\"unknown\",\n",
        "    environment=\"unknown\"\n",
        "):\n",
        "    \"\"\"Generate diagnosis and fix for pipeline failure\"\"\"\n",
        "    \n",
        "    # Build the prompt in the same format as training data\n",
        "    prompt = f\"\"\"Analyze this CI/CD pipeline failure and provide a solution:\n",
        "\n",
        "Pipeline ID: {pipeline_id}\n",
        "Stage: {stage_name}\n",
        "Job: {job_name}\n",
        "Task: {task_name}\n",
        "Branch: {branch}\n",
        "Status: {status}\n",
        "Error: {message}\n",
        "\n",
        "What went wrong and how should we fix it?\"\"\"\n",
        "    \n",
        "    # Apply chat template if available\n",
        "    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            messages, \n",
        "            tokenize=False, \n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    else:\n",
        "        formatted_prompt = prompt\n",
        "    \n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=400,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode the full response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Try to extract just the assistant's response\n",
        "    # Method 1: Split by the prompt\n",
        "    if \"What went wrong and how should we fix it?\" in full_response:\n",
        "        response = full_response.split(\"What went wrong and how should we fix it?\")[-1].strip()\n",
        "    # Method 2: Split by assistant marker if present\n",
        "    elif \"<|assistant|>\" in full_response:\n",
        "        response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
        "    # Method 3: Just remove the input prompt\n",
        "    else:\n",
        "        response = full_response.replace(formatted_prompt, \"\").strip()\n",
        "    \n",
        "    # If response is empty, return the full output for debugging\n",
        "    if not response or len(response) < 10:\n",
        "        response = full_response\n",
        "    \n",
        "    return response\n",
        "\n",
        "# 3. Test with examples from your actual data\n",
        "print(\"üß™ Testing with real pipeline failures from your dataset:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Example 1: Test failure from your data\n",
        "print(\"\\nüìã TEST 1: Test Stage Failure\")\n",
        "print(\"-\" * 80)\n",
        "try:\n",
        "    diagnosis = diagnose_pipeline_failure(\n",
        "        pipeline_id=\"pipe-mwkkl\",\n",
        "        stage_name=\"Test\",\n",
        "        job_name=\"build_and_test\",\n",
        "        task_name=\"test\",\n",
        "        branch=\"branch_xqp\",\n",
        "        status=\"failed\",\n",
        "        message=\"Task execution failed.\",\n",
        "        commit_id=\"a4e872fb\"\n",
        "    )\n",
        "    print(f\"Response length: {len(diagnosis)} chars\\n\")\n",
        "    print(diagnosis)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Example 2: Build failure\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nüìã TEST 2: Build Stage Failure\")\n",
        "print(\"-\" * 80)\n",
        "try:\n",
        "    diagnosis = diagnose_pipeline_failure(\n",
        "        pipeline_id=\"pipe-abc123\",\n",
        "        stage_name=\"Build\",\n",
        "        job_name=\"compile_project\",\n",
        "        task_name=\"build\",\n",
        "        branch=\"feature/new-api\",\n",
        "        status=\"failed\",\n",
        "        message=\"Compilation error: undefined reference to 'parseConfig'\",\n",
        "        commit_id=\"def456gh\"\n",
        "    )\n",
        "    print(f\"Response length: {len(diagnosis)} chars\\n\")\n",
        "    print(diagnosis)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Example 3: Deployment failure\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nüìã TEST 3: Deployment Failure\")\n",
        "print(\"-\" * 80)\n",
        "try:\n",
        "    diagnosis = diagnose_pipeline_failure(\n",
        "        pipeline_id=\"pipe-deploy-99\",\n",
        "        stage_name=\"Deploy\",\n",
        "        job_name=\"deploy_to_production\",\n",
        "        task_name=\"deploy\",\n",
        "        branch=\"main\",\n",
        "        status=\"failed\",\n",
        "        message=\"Permission denied: unable to access deployment credentials\",\n",
        "        environment=\"production\"\n",
        "    )\n",
        "    print(f\"Response length: {len(diagnosis)} chars\\n\")\n",
        "    print(diagnosis)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\n‚úÖ Testing complete!\")\n",
        "print(\"\\nüí° To test with your own failures, use:\")\n",
        "print(\"   diagnosis = diagnose_pipeline_failure(pipeline_id, stage, job, task, branch, status, message)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä TESTING MODEL WITH REAL CI/CD FAILURES\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "TEST 1/5\n",
            "------------------------------------------------------------------------------------------\n",
            "üîç FAILURE DETAILS:\n",
            "   Pipeline: pipe-gvqcj\n",
            "   Stage: Test ‚Üí Job: run_integration_tests ‚Üí Task: checkout\n",
            "   Branch: branch_lou\n",
            "   Error: Task execution failed.\n",
            "\n",
            "ü§ñ MODEL DIAGNOSIS:\n",
            "\n",
            "assistant\n",
            "Tests failed in the Test stage.\n",
            "\n",
            "Recommended fixes:\n",
            "1. Review the test failure logs for specific assertions\n",
            "2. Check if recent code changes in commit 983090d4 broke functionality\n",
            "3. Verify test data and mocks are properly configured\n",
            "4. Run tests locally to reproduce and debug\n",
            "5. Check for environment-specific issues in nan\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "TEST 2/5\n",
            "------------------------------------------------------------------------------------------\n",
            "üîç FAILURE DETAILS:\n",
            "   Pipeline: pipe-bmxgt\n",
            "   Stage: Test ‚Üí Job: run_unit_tests ‚Üí Task: deploy\n",
            "   Branch: branch_xnw\n",
            "   Error: Task execution failed.\n",
            "\n",
            "ü§ñ MODEL DIAGNOSIS:\n",
            "\n",
            "assistant\n",
            "Tests failed in the Test stage.\n",
            "\n",
            "Recommended fixes:\n",
            "1. Review the test failure logs for specific assertions\n",
            "2. Check if recent code changes in commit 3b3d89c2 broke functionality\n",
            "3. Verify test data and mocks are properly configured\n",
            "4. Run tests locally to reproduce and debug\n",
            "5. Check for environment-specific issues in nan\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "TEST 3/5\n",
            "------------------------------------------------------------------------------------------\n",
            "üîç FAILURE DETAILS:\n",
            "   Pipeline: pipe-jgvxp\n",
            "   Stage: Deploy ‚Üí Job: deploy_to_staging ‚Üí Task: build\n",
            "   Branch: branch_yjd\n",
            "   Error: Task execution failed.\n",
            "\n",
            "ü§ñ MODEL DIAGNOSIS:\n",
            "\n",
            "assistant\n",
            "Tests failed in the Deploy stage.\n",
            "\n",
            "Recommended fixes:\n",
            "1. Review the test failure logs for specific assertions\n",
            "2. Check if recent code changes in commit 5d2c1d09 broke functionality\n",
            "3. Verify test data and mocks are properly configured\n",
            "4. Run tests locally to reproduce and debug\n",
            "5. Check for environment-specific issues in nan\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "TEST 4/5\n",
            "------------------------------------------------------------------------------------------\n",
            "üîç FAILURE DETAILS:\n",
            "   Pipeline: pipe-vacaa\n",
            "   Stage: Analysis ‚Üí Job: run_unit_tests ‚Üí Task: deploy\n",
            "   Branch: branch_rqs\n",
            "   Error: Task execution failed.\n",
            "\n",
            "ü§ñ MODEL DIAGNOSIS:\n",
            "\n",
            "assistant\n",
            "Tests failed in the Analysis stage.\n",
            "\n",
            "Recommended fixes:\n",
            "1. Review the test failure logs for specific assertions\n",
            "2. Check if recent code changes in commit 8e7c7e5a broke functionality\n",
            "3. Verify test data and mocks are properly configured\n",
            "4. Run tests locally to reproduce and debug\n",
            "5. Check for environment-specific issues in nan\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "TEST 5/5\n",
            "------------------------------------------------------------------------------------------\n",
            "üîç FAILURE DETAILS:\n",
            "   Pipeline: pipe-etxqd\n",
            "   Stage: Build ‚Üí Job: build_and_test ‚Üí Task: deploy\n",
            "   Branch: branch_kxo\n",
            "   Error: Task execution failed.\n",
            "\n",
            "ü§ñ MODEL DIAGNOSIS:\n",
            "\n",
            "assistant\n",
            "Tests failed in the Build stage.\n",
            "\n",
            "Recommended fixes:\n",
            "1. Review the test failure logs for specific assertions\n",
            "2. Check if recent code changes in commit 92cbc50f broke functionality\n",
            "3. Verify test data and mocks are properly configured\n",
            "4. Run tests locally to reproduce and debug\n",
            "5. Check for environment-specific issues in nan\n",
            "\n",
            "==========================================================================================\n",
            "\n",
            "\n",
            "==========================================================================================\n",
            "üìà TESTING SUMMARY\n",
            "==========================================================================================\n",
            "\n",
            "Total tests: 5\n",
            "Average diagnosis length: 335 chars\n",
            "Tests with recommendations: 5/5\n",
            "\n",
            "üìã Breakdown by stage:\n",
            " test_num    stage                   job  diagnosis_length\n",
            "        1     Test run_integration_tests               334\n",
            "        2     Test        run_unit_tests               334\n",
            "        3   Deploy     deploy_to_staging               336\n",
            "        4 Analysis        run_unit_tests               338\n",
            "        5    Build        build_and_test               335\n",
            "\n",
            "‚úÖ Model testing complete!\n"
          ]
        }
      ],
      "source": [
        "def diagnose_and_display(\n",
        "    pipeline_id, stage_name, job_name, task_name, \n",
        "    branch, status, message, commit_id=\"unknown\", environment=\"unknown\"\n",
        "):\n",
        "    \"\"\"Diagnose and display with clean formatting\"\"\"\n",
        "    \n",
        "    print(f\"üîç FAILURE DETAILS:\")\n",
        "    print(f\"   Pipeline: {pipeline_id}\")\n",
        "    print(f\"   Stage: {stage_name} ‚Üí Job: {job_name} ‚Üí Task: {task_name}\")\n",
        "    print(f\"   Branch: {branch}\")\n",
        "    print(f\"   Error: {message}\")\n",
        "    print(f\"\\nü§ñ MODEL DIAGNOSIS:\\n\")\n",
        "    \n",
        "    diagnosis = diagnose_pipeline_failure(\n",
        "        pipeline_id, stage_name, job_name, task_name,\n",
        "        branch, status, message, commit_id, environment\n",
        "    )\n",
        "    \n",
        "    # Clean up the response\n",
        "    diagnosis = diagnosis.replace(\"assistant \", \"\").strip()\n",
        "    print(diagnosis)\n",
        "    print(\"\\n\" + \"=\" * 90 + \"\\n\")\n",
        "    \n",
        "    return diagnosis\n",
        "\n",
        "# Test with your CSV data\n",
        "print(\"üìä TESTING MODEL WITH REAL CI/CD FAILURES\\n\")\n",
        "print(\"=\" * 90 + \"\\n\")\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv(\"./ci_cd_logs.csv\")\n",
        "failed_logs = df[df['status'].str.lower() == 'failed'].copy()\n",
        "\n",
        "# Test with 5 random failures\n",
        "test_samples = failed_logs.sample(min(5, len(failed_logs)))\n",
        "\n",
        "results = []\n",
        "for idx, (_, row) in enumerate(test_samples.iterrows(), 1):\n",
        "    print(f\"TEST {idx}/5\")\n",
        "    print(\"-\" * 90)\n",
        "    \n",
        "    diagnosis = diagnose_and_display(\n",
        "        pipeline_id=row['pipeline_id'],\n",
        "        stage_name=row['stage_name'],\n",
        "        job_name=row['job_name'],\n",
        "        task_name=row['task_name'],\n",
        "        branch=row['branch'],\n",
        "        status=row['status'],\n",
        "        message=row['message'],\n",
        "        commit_id=str(row.get('commit_id', 'unknown'))[:8],\n",
        "        environment=str(row.get('environment', 'unknown'))\n",
        "    )\n",
        "    \n",
        "    results.append({\n",
        "        'test_num': idx,\n",
        "        'stage': row['stage_name'],\n",
        "        'job': row['job_name'],\n",
        "        'error': row['message'][:50] + \"...\",\n",
        "        'diagnosis_length': len(diagnosis),\n",
        "        'has_recommendations': 'Recommended fixes:' in diagnosis or '1.' in diagnosis\n",
        "    })\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"üìà TESTING SUMMARY\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "summary_df = pd.DataFrame(results)\n",
        "print(f\"\\nTotal tests: {len(results)}\")\n",
        "print(f\"Average diagnosis length: {summary_df['diagnosis_length'].mean():.0f} chars\")\n",
        "print(f\"Tests with recommendations: {summary_df['has_recommendations'].sum()}/{len(results)}\")\n",
        "\n",
        "print(\"\\nüìã Breakdown by stage:\")\n",
        "print(summary_df[['test_num', 'stage', 'job', 'diagnosis_length']].to_string(index=False))\n",
        "\n",
        "print(\"\\n‚úÖ Model testing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading BASE model for comparison...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kcodess/venvs/ai310/lib/python3.10/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Base model loaded!\n",
            "\n",
            "==========================================================================================\n",
            "üß™ COMPARISON TEST: Integration Test Timeout\n",
            "==========================================================================================\n",
            "\n",
            "ü§ñ YOUR FINE-TUNED MODEL:\n",
            "------------------------------------------------------------------------------------------\n",
            "assistant\n",
            "Tests failed in the Test stage.\n",
            "\n",
            "Recommended fixes:\n",
            "1. Review the test failure logs for specific assertions\n",
            "2. Check if recent code changes in commit 9c782cbe broke functionality\n",
            "3. Verify test data and mocks are properly configured\n",
            "4. Run tests locally to reproduce and debug\n",
            "5. Check for environment-specific issues in nan\n",
            "\n",
            "\n",
            "üìò BASE MODEL (not fine-tuned):\n",
            "------------------------------------------------------------------------------------------\n",
            "assistant\n",
            "The pipeline failed due to a bug in the `test_integration_tests` test suite. Specifically, the `test_integration_tests` test suite failed to complete after 15 minutes.\n",
            "\n",
            "The issue lies in the `run_integration_tests` test, which includes a test suite that includes a test called `test_integration_tests`. The `test_integration_tests` test suite includes a test named `run_integration_tests`.\n",
            "\n",
            "Here's a breakdown of the problem:\n",
            "\n",
            "1. The `run_integration_tests` test suite includes a test named `test_integration_tests`.\n",
            "2. The `test_integration_tests` test suite includes a test named `run_integration_tests`.\n",
            "3. When the pipeline completes, the pipeline's `run_integration_tests` test suite will fail.\n",
            "\n",
            "To fix this issue, we need to modify the test suite to include a test named `test_integration_tests`. We can add the following code to the test suite:\n",
            "\n",
            "```python\n",
            "import unittest\n",
            "\n",
            "class TestIntegrationTests(unittest.TestCase):\n",
            "    def test_integration_tests(self):\n",
            "        self.assertEqual(self.run_integration_tests(\"run_integration_tests\"), \"Test suite timed out after 15 minutes\")\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    unittest.main()\n",
            "```\n",
            "\n",
            "By adding the `TestIntegrationTests` test, the pipeline will now complete without any test failures.\n",
            "\n",
            "\n",
            "==========================================================================================\n",
            "üìä COMPARISON METRICS\n",
            "==========================================================================================\n",
            "Fine-tuned response length: 334 chars\n",
            "Base model response length: 1247 chars\n",
            "\n",
            "Fine-tuned has structured fixes: True\n",
            "Base model has structured fixes: True\n",
            "\n",
            "Fine-tuned mentions specifics: True\n",
            "Base model mentions specifics: True\n"
          ]
        }
      ],
      "source": [
        "# Compare your fine-tuned model with the base model\n",
        "\n",
        "\n",
        "\n",
        "print(\"üîÑ Loading BASE model for comparison...\")\n",
        "base_model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "if base_tokenizer.pad_token is None:\n",
        "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "print(\"‚úÖ Base model loaded!\\n\")\n",
        "\n",
        "def get_base_model_response(prompt):\n",
        "    \"\"\"Get response from base model\"\"\"\n",
        "    if hasattr(base_tokenizer, 'chat_template') and base_tokenizer.chat_template:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        formatted_prompt = base_tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    else:\n",
        "        formatted_prompt = prompt\n",
        "    \n",
        "    inputs = base_tokenizer(formatted_prompt, return_tensors=\"pt\").to(base_model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=400,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=base_tokenizer.pad_token_id,\n",
        "            eos_token_id=base_tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    full_response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract response\n",
        "    if \"What went wrong and how should we fix it?\" in full_response:\n",
        "        response = full_response.split(\"What went wrong and how should we fix it?\")[-1].strip()\n",
        "    elif \"<|assistant|>\" in full_response:\n",
        "        response = full_response.split(\"<|assistant|>\")[-1].strip()\n",
        "    else:\n",
        "        response = full_response.replace(formatted_prompt, \"\").strip()\n",
        "    \n",
        "    return response.replace(\"assistant \", \"\").strip()\n",
        "\n",
        "# Test case\n",
        "test_prompt = \"\"\"Analyze this CI/CD pipeline failure and provide a solution:\n",
        "\n",
        "Pipeline ID: pipe-test-001\n",
        "Stage: Test\n",
        "Job: run_integration_tests\n",
        "Task: test\n",
        "Branch: feature/auth\n",
        "Status: failed\n",
        "Error: Test suite timed out after 15 minutes\n",
        "\n",
        "What went wrong and how should we fix it?\"\"\"\n",
        "\n",
        "print(\"=\" * 90)\n",
        "print(\"üß™ COMPARISON TEST: Integration Test Timeout\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "print(\"\\nü§ñ YOUR FINE-TUNED MODEL:\")\n",
        "print(\"-\" * 90)\n",
        "finetuned_response = diagnose_pipeline_failure(\n",
        "    \"pipe-test-001\", \"Test\", \"run_integration_tests\", \"test\",\n",
        "    \"feature/auth\", \"failed\", \"Test suite timed out after 15 minutes\"\n",
        ").replace(\"assistant \", \"\").strip()\n",
        "print(finetuned_response)\n",
        "\n",
        "print(\"\\n\\nüìò BASE MODEL (not fine-tuned):\")\n",
        "print(\"-\" * 90)\n",
        "base_response = get_base_model_response(test_prompt)\n",
        "print(base_response)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\" * 90)\n",
        "print(\"üìä COMPARISON METRICS\")\n",
        "print(\"=\" * 90)\n",
        "print(f\"Fine-tuned response length: {len(finetuned_response)} chars\")\n",
        "print(f\"Base model response length: {len(base_response)} chars\")\n",
        "print(f\"\\nFine-tuned has structured fixes: {'Recommended fixes:' in finetuned_response or '1.' in finetuned_response}\")\n",
        "print(f\"Base model has structured fixes: {'Recommended fixes:' in base_response or '1.' in base_response}\")\n",
        "print(f\"\\nFine-tuned mentions specifics: {any(word in finetuned_response.lower() for word in ['timeout', 'test', 'pipeline'])}\")\n",
        "print(f\"Base model mentions specifics: {any(word in base_response.lower() for word in ['timeout', 'test', 'pipeline'])}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
